{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Ensemble Learning"
      ],
      "metadata": {
        "id": "l6qG4-gybXZG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is Ensemble Learning in machine learning? Explain the key idea behind it ?\n",
        "\n",
        "Ans- Ensemble Learning is a machine learning technique where multiple models (called base learners or weak learners) are trained and their predictions are combined to produce a better overall model.\n",
        "\n",
        "\n",
        "Key Idea Behind Ensemble Learning\n",
        "- A group of diverse models, when combined intelligently, performs better than any single model.\n",
        "\n",
        "Reducces Variance\n",
        "\n",
        "- Some models (e.g., Decision Trees) are sensitive to data changes\n",
        "\n",
        "- Combining many such models stabilizes predictions\n",
        "\n",
        "Reduces Bias\n",
        "\n",
        "- Weak models can be combined to form a strong model\n",
        "\n",
        "Improves Accuracy & Robustness\n",
        "\n",
        " - Errors made by one model may be corrected by others"
      ],
      "metadata": {
        "id": "W6reLcZkbZB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. What is the difference between Bagging and Boosting?\n",
        "\n",
        "Ans-\n",
        "\n",
        "| Aspect             | **Bagging (Bootstrap Aggregating)**                               | **Boosting**                                                       |\n",
        "| ------------------ | ----------------------------------------------------------------- | ------------------------------------------------------------------ |\n",
        "| Basic idea         | Builds **independent models** on different random subsets of data | Builds models **sequentially**, each correcting previous errors    |\n",
        "| Data sampling      | Uses **bootstrap sampling** (with replacement)                    | Uses **weighted sampling**, focusing more on misclassified samples |\n",
        "| Model dependency   | Models are **independent** of each other                          | Models are **dependent** on previous models                        |\n",
        "| Goal               | Reduce **variance**                                               | Reduce **bias** (and variance in some cases)                       |\n",
        "| Handling errors    | All samples treated **equally**                                   | Misclassified samples get **higher importance**                    |\n",
        "| Overfitting        | Helps prevent overfitting                                         | Can **overfit** if too many iterations                             |\n",
        "| Noise sensitivity  | Less sensitive to noise                                           | More sensitive to noisy data and outliers                          |\n",
        "| Parallel training  | Yes (models can be trained in parallel)                           | No (models trained one after another)                              |\n",
        "| Typical base model | High-variance models (e.g., Decision Trees)                       | Weak learners (e.g., Decision Stumps)                              |\n",
        "| Example algorithms | Random Forest                                                     | AdaBoost, Gradient Boosting, XGBoost                               |\n"
      ],
      "metadata": {
        "id": "09MteMs-bZE4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?\n",
        "\n",
        "Ans- Bootstrap sampling is a resampling technique where multiple new training datasets are created by randomly sampling from the original dataset with replacement.\n",
        "\n",
        "Each bootstrap sample has the same size as the original dataset, but some records may appear multiple times, while others may not appear at all.\n",
        "\n",
        "Role of Bootstrap Sampling in Bagging\n",
        "\n",
        "Creates Diversity Among Models\n",
        "\n",
        "- Each tree is trained on a different bootstrap dataset\n",
        "\n",
        "- Different data → different trees → model diversity\n",
        "\n",
        "- Diversity is essential for an effective ensemble\n",
        "\n",
        "Reduces Variance\n",
        "\n",
        "- Individual decision trees are high-variance models\n",
        "\n",
        "- Averaging predictions from many trees trained on bootstrap samples stabilizes results\n",
        "\n",
        "- Helps prevent overfitting\n",
        "\n",
        "Enables Out-of-Bag (OOB) Error Estimation\n",
        "\n",
        "- Data points not selected in a bootstrap sample (~37%) are used as validation data\n",
        "\n",
        "- OOB error provides a built-in accuracy estimate without a separate test set\n",
        "\n",
        "Foundation of Random Forest\n",
        "\n",
        "In Random Forest:\n",
        "\n",
        "- Bootstrap sampling selects different training data for each tree\n",
        "\n",
        "- Random feature selection further increases diversity\n",
        "\n",
        "- Together, they make Random Forest robust and accurate"
      ],
      "metadata": {
        "id": "ly9ki2UGbZQG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?\n",
        "\n",
        "Ans- Out-of-Bag (OOB) samples are the data points that are not selected in a bootstrap sample when training a model in Bagging-based ensemble methods such as Random Forest.\n",
        "\n",
        "For each tree in a Random Forest:\n",
        "\n",
        "- A bootstrap sample is drawn from the dataset\n",
        "\n",
        "- Some observations are not selected at all\n",
        "\n",
        "- These unselected observations are the OOB samples for that tree\n",
        "\n",
        "Step-by-Step Process:\n",
        "\n",
        "1)  For a given data point, collect predictions only from trees where this point was OOB\n",
        "\n",
        "2) Combine these predictions:\n",
        "\n",
        "- Majority vote (classification)\n",
        "\n",
        "- Average (regression)\n",
        "\n",
        "3) Compare the combined prediction with the true label\n",
        "\n",
        "4) Repeat for all data points\n",
        "\n",
        "5) Compute overall accuracy (or error)\n"
      ],
      "metadata": {
        "id": "Y8-rl5DkbZWG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. Compare feature importance analysis in a single Decision Tree vs. a Random Forest.\n",
        "\n",
        "Ans-\n",
        "\n",
        "| Aspect                       | **Single Decision Tree**                                                                   | **Random Forest**                                                 |\n",
        "| ---------------------------- | ------------------------------------------------------------------------------------------ | ----------------------------------------------------------------- |\n",
        "| How importance is calculated | Based on **total reduction in impurity** (Gini/Entropy/MSE) from splits using that feature | **Averaged impurity reduction** of a feature across **all trees** |\n",
        "| Stability                    | **Unstable** (changes with small data variations)                                          | **More stable and reliable**                                      |\n",
        "| Bias                         | Can be **biased toward features with many split points**                                   | Bias is **reduced but not eliminated**                            |\n",
        "| Overfitting effect           | High risk of overfitting → misleading importance                                           | Lower overfitting → more trustworthy importance                   |\n",
        "| Use of data                  | Uses **entire dataset once**                                                               | Uses **multiple bootstrap samples**                               |\n",
        "| Feature interaction capture  | Limited (single structure)                                                                 | Better capture of **feature interactions**                        |\n",
        "| Sensitivity to noise         | Highly sensitive                                                                           | More robust to noisy features                                     |\n",
        "| Generalization               | Poorer generalization                                                                      | Better generalization                                             |\n",
        "| Example output               | One feature may dominate strongly                                                          | Importance is **distributed** across features                     |\n"
      ],
      "metadata": {
        "id": "BL3r-djObXb9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. Write a Python program to:\n",
        "● Load the Breast Cancer dataset using\n",
        "sklearn.datasets.load_breast_cancer()\n",
        "● Train a Random Forest Classifier\n",
        "● Print the top 5 most important features based on feature importance scores.\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n"
      ],
      "metadata": {
        "id": "UYfcomy5bXee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ans Q6.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "\n",
        "importances = rf.feature_importances_\n",
        "\n",
        "\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Print top 5 most important features\n",
        "print(feature_importance_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxA9she2hiFe",
        "outputId": "8c578196-6252-4d02-f483-0debb3f4dae9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 Feature  Importance\n",
            "23            worst area    0.139357\n",
            "27  worst concave points    0.132225\n",
            "7    mean concave points    0.107046\n",
            "20          worst radius    0.082848\n",
            "22       worst perimeter    0.080850\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. Write a Python program to:\n",
        "● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "● Evaluate its accuracy and compare with a single Decision Tree"
      ],
      "metadata": {
        "id": "CzY-te0KbXhe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ans Q7.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "dt_predictions = dt.predict(X_test)\n",
        "dt_accuracy = accuracy_score(y_test, dt_predictions)\n",
        "\n",
        "# Train a Bagging Classifier using Decision Trees\n",
        "bagging = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "bagging.fit(X_train, y_train)\n",
        "bagging_predictions = bagging.predict(X_test)\n",
        "bagging_accuracy = accuracy_score(y_test, bagging_predictions)\n",
        "\n",
        "# Print accuracies\n",
        "print(\"Decision Tree Accuracy:\", dt_accuracy)\n",
        "print(\"Bagging Classifier Accuracy:\", bagging_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWj3TSH5h-we",
        "outputId": "29b711f1-00d4-4e23-c981-5a4acd9fc550"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 1.0\n",
            "Bagging Classifier Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. Write a Python program to:\n",
        "● Train a Random Forest Classifier\n",
        "● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "● Print the best parameters and final accuracy"
      ],
      "metadata": {
        "id": "EumHgRAphwvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ans Q8.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize Random Forest classifier\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [None, 5, 10]\n",
        "}\n",
        "\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy'\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model\n",
        "best_rf = grid_search.best_estimator_\n",
        "\n",
        "\n",
        "y_pred = best_rf.predict(X_test)\n",
        "final_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print best parameters and final accuracy\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Final Accuracy:\", final_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d01gdUk-iQEP",
        "outputId": "ab85aa5e-9620-4aa8-a690-2683582c3c27"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 5, 'n_estimators': 150}\n",
            "Final Accuracy: 0.9707602339181286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9. Write a Python program to:\n",
        "● Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "Housing dataset\n",
        "● Compare their Mean Squared Errors (MSE)"
      ],
      "metadata": {
        "id": "Dw7smxSJifYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ans Q9.\n",
        "\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "bagging_regressor = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "bagging_regressor.fit(X_train, y_train)\n",
        "y_pred_bagging = bagging_regressor.predict(X_test)\n",
        "\n",
        "\n",
        "mse_bagging = mean_squared_error(y_test, y_pred_bagging)\n",
        "\n",
        "\n",
        "random_forest_regressor = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "random_forest_regressor.fit(X_train, y_train)\n",
        "y_pred_rf = random_forest_regressor.predict(X_test)\n",
        "\n",
        "# Calculate MSE for Random Forest Regressor\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "\n",
        "\n",
        "print(\"Mean Squared Error (Bagging Regressor):\", mse_bagging)\n",
        "print(\"Mean Squared Error (Random Forest Regressor):\", mse_rf)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rh3igIutiqQw",
        "outputId": "54cc81c7-92a7-4775-f6f7-332b20933eaf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (Bagging Regressor): 0.25592438609899626\n",
            "Mean Squared Error (Random Forest Regressor): 0.2553684927247781\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10. You are working as a data scientist at a financial institution to predict loan\n",
        "default. You have access to customer demographic and transaction history data.\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "Explain your step-by-step approach to:\n",
        "- Choose between Bagging or Boosting\n",
        "- Handle overfitting\n",
        "- Select base models\n",
        "- Evaluate performance using cross-validation\n",
        "- Justify how ensemble learning improves decision-making in this real-world\n",
        "context\n",
        "\n",
        "Ans-\n",
        "\n",
        "1. Choosing Between Bagging and Boosting\n",
        "\n",
        "Understanding the Problem\n",
        "\n",
        "- Loan default data is usually imbalanced, noisy, and non-linear\n",
        "\n",
        "- False negatives (missing a defaulter) are costly\n",
        "\n",
        "Decision Strategy\n",
        "\n",
        "- Bagging reduces variance and is robust to noise\n",
        "\n",
        "- Boosting focuses on hard-to-classify samples and reduces bias\n",
        "\n",
        "Choice\n",
        "\n",
        "- Start with Bagging (Random Forest) for stability\n",
        "\n",
        "- Use Boosting (Gradient Boosting / AdaBoost) to improve performance\n",
        "\n",
        "In financial institutions, Random Forest is preferred initially due to stability and explainability.\n",
        "\n",
        "2. Handling Overfitting\n",
        "\n",
        "Overfitting can lead to risky credit decisions.\n",
        "\n",
        "Techniques Used\n",
        "\n",
        "- Limit tree depth (max_depth)\n",
        "\n",
        "- Set minimum samples per leaf\n",
        "\n",
        "- Use ensemble averaging (Bagging)\n",
        "\n",
        "- Use cross-validation\n",
        "\n",
        "- Use early stopping (Boosting)\n",
        "\n",
        "3. Selecting Base Models\n",
        "\n",
        "Criteria\n",
        "\n",
        "- Handle non-linear patterns\n",
        "\n",
        "- Work well with mixed data types\n",
        "\n",
        "- Easy to interpret\n",
        "\n",
        "Selected Base Models\n",
        "\n",
        "- Decision Trees (for Bagging & Boosting)\n",
        "\n",
        "- Logistic Regression (benchmark & explainability)\n",
        "\n",
        "Decision Trees are chosen because:\n",
        "\n",
        "- They capture interactions in demographic & transaction data\n",
        "\n",
        "- Ensembles of trees reduce individual model weaknesses\n",
        "\n",
        "4. Performance Evaluation using Cross-Validation\n",
        "\n",
        "Why Cross-Validation?\n",
        "\n",
        "- Prevents biased evaluation\n",
        "\n",
        "- Ensures model stability\n",
        "\n",
        "Method\n",
        "\n",
        "- Stratified K-Fold Cross-Validation\n",
        "\n",
        "- Maintains default / non-default ratio\n",
        "\n",
        "Metrics Used\n",
        "\n",
        "- ROC-AUC → Ranking default risk\n",
        "\n",
        "- Accuracy → Overall correctness\n",
        "\n",
        "5. Why Ensemble Learning Improves Decision-Making\n",
        "\n",
        "Business Justification\n",
        "\n",
        "- Combines multiple models → lower risk\n",
        "\n",
        "- More stable predictions for high-value loans\n",
        "\n",
        "- Handles noisy transactional data\n",
        "\n",
        "- Improves approval quality\n",
        "\n",
        "- Supports regulatory explainability (feature importance, SHAP)"
      ],
      "metadata": {
        "id": "x9XSw6TQi-qw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ans Q10.\n",
        "\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "\n",
        "X, y = make_classification(\n",
        "    n_samples=5000,\n",
        "    n_features=20,\n",
        "    n_informative=10,\n",
        "    n_redundant=5,\n",
        "    weights=[0.85, 0.15],   # Imbalanced dataset\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=6,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rf_auc = cross_val_score(\n",
        "    rf_model, X, y, cv=cv, scoring=\"roc_auc\"\n",
        ")\n",
        "\n",
        "\n",
        "gb_model = GradientBoostingClassifier(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=3,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "gb_auc = cross_val_score(\n",
        "    gb_model, X, y, cv=cv, scoring=\"roc_auc\"\n",
        ")\n",
        "\n",
        "# Print Results\n",
        "print(\"Random Forest Mean ROC-AUC:\", rf_auc.mean())\n",
        "print(\"Gradient Boosting Mean ROC-AUC:\", gb_auc.mean())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_f3NBP6cjry2",
        "outputId": "526e8f97-d574-43f1-8b80-6666cd1733b7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Mean ROC-AUC: 0.9400632464462253\n",
            "Gradient Boosting Mean ROC-AUC: 0.9509973193896281\n"
          ]
        }
      ]
    }
  ]
}